import time

import torch
from transformers import (
    BitsAndBytesConfig,
    GenerationConfig,
    LlamaTokenizer,
    MistralForCausalLM,
    TextStreamer,
)

from torch_bnb_fp4 import recursively_replace_with_fp4_linear

# Change this to your desired dtype
DTYPE = torch.float16

model_path = "mistralai/Mistral-7B-Instruct-v0.2"

# Load weights as bnb fp4
model: MistralForCausalLM = MistralForCausalLM.from_pretrained(
    model_path,
    torch_dtype=DTYPE,
    quantization_config=BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=DTYPE,
        # Must use "fp4" for this library
        bnb_4bit_quant_type="fp4",
        # double quant is also unsupported, set this to false
        bnb_4bit_use_double_quant=False,
    ),
)

tokenizer: LlamaTokenizer = LlamaTokenizer.from_pretrained(model_path)
tokenizer.pad_token_id = tokenizer.eos_token_id
model.config.pad_token_id = tokenizer.eos_token_id

silly_example = (
    "I want to create sentences which sound correct and are grammatically correct, but don't make any sense such as the following:\n"
    + "Now basically the only new principle involved is that instead of power being generated by the relative motion of conductors and fluxes, "
    + "it's produced by the modial interaction of magneto reluctance and capacitive duractants. The original machine had a base plate of pre-famulated amulite, "
    + "surmounted by a malleable logarithmic casing in such a way that the two spurving bearings were in a direct line with a panometric fam. "
    + "The lineup consisted simply of six hydroscoptic marsal veins so fitted to the ambifacent lunar wane shaft that side fumbling was effectively prevented. "
    + "The main winding was of the normal lotus-o-deltoid type placed in panendermic semi-boloid slots of the stator. "
    + "Every seventh conductor being connected by a non-reversible tremi pipe to the differential girdle spring on the up end of the gram meters. "
    + "Moreover, whenever fluorescent score motion is required, it may also be employed in conjunction with a drawn reciprocation dingle arm to reduce sinusoidal depleneration.\n"
    + "Could you help make a sentence that is grammatically correct but doesn't make any sense using overly verbose language relating to some new technology?\n"
)

ctx = tokenizer.apply_chat_template(
    [{"role": "user", "content": silly_example}],
    add_generation_prompt=True,
    tokenize=True,
    return_tensors="pt",
).to(model.device)

gen_kwargs = GenerationConfig(
    **{
        "max_new_tokens": 256,
        "min_new_tokens": 255,
        "temperature": 0.78,
        "do_sample": True,
        "top_k": 40,
        "top_p": 0.9,
        "num_return_sequences": 1,
        "use_cache": True,
    }
)

streamer = TextStreamer(tokenizer)

with torch.inference_mode():
    st = time.perf_counter()
    out = model.generate(
        ctx,
        generation_config=gen_kwargs,
        pad_token_id=tokenizer.eos_token_id,
    )
    nd = time.perf_counter() - st
    print("\nRun # 1 (warmup) BNB\n")
    print("Time to generate: ", nd)
    print("Total new tokens: ", len(out[0]) - ctx.shape[1])
    print("Total ctx+tokens: ", len(out[0]))
    print("Generation tok/s: ", (len(out[0]) - ctx.shape[1]) / nd, "\n")
    st = time.perf_counter()
    out = model.generate(
        ctx,
        generation_config=gen_kwargs,
        pad_token_id=tokenizer.eos_token_id,
    )
    nd = time.perf_counter() - st
    print("\nRun # 2 BNB\n")
    print("Time to generate: ", nd)
    print("Total new tokens: ", len(out[0]) - ctx.shape[1])
    print("Total ctx+tokens: ", len(out[0]))
    print("Generation tok/s: ", (len(out[0]) - ctx.shape[1]) / nd, "\n")


# # # # Replace layers with torch-bnb-fp4 layers in-place
recursively_replace_with_fp4_linear(
    model,
    as_dtype=DTYPE,
    use_codebook_dequant=True,  # or False for fp4 tree dequant, though doesn't make much difference.
    only_replace_bnb_layers=True,
)

with torch.inference_mode():
    st = time.perf_counter()
    out = model.generate(
        ctx,
        generation_config=gen_kwargs,
        pad_token_id=tokenizer.eos_token_id,
    )
    nd = time.perf_counter() - st
    print("\nRun # 1 (warmup) TorchFP4\n")
    print("Time to generate: ", nd)
    print("Total new tokens: ", len(out[0]) - ctx.shape[1])
    print("Total ctx+tokens: ", len(out[0]))
    print("Generation tok/s: ", (len(out[0]) - ctx.shape[1]) / nd, "\n")
    st = time.perf_counter()
    out = model.generate(
        ctx,
        generation_config=gen_kwargs,
        pad_token_id=tokenizer.eos_token_id,
    )
    nd = time.perf_counter() - st
    print("\nRun # 2 TorchFP4\n")
    print("Time to generate: ", nd)
    print("Total new tokens: ", len(out[0]) - ctx.shape[1])
    print("Total ctx+tokens: ", len(out[0]))
    print("Generation tok/s: ", (len(out[0]) - ctx.shape[1]) / nd, "\n")
